---
title: "NotebookLab4"
author: "Thomas Maria Frey"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Set your working directory and load the required librarys.
```{r}
setwd("~/Desktop/University/CSN-MIRI Complex and Social Networks/Lab/Lab4")
```

1. Introduction 
We are part of the <k^2> group. k^2 is defined as its degree 2nd moment. We 
investigate the scaling of <k^2> as a function of n.

2. Data Preparation
We are reading in the information from the dependency_tree_metrics.tar.gz file.
The rows of this dataframe contains three columns: n, <k^2> and <d>.
Here we check the validity of <k^2> and <d> and we create a table containing,
for each language, that contains the sample size n, the mean and standard 
deviation of n and the mean and the standard deviation of our target metric
<k^2>.
Note: Not all languages satisfy the metrics set forth in 2.(1)!
```{r}
# Load necessary libraries
library(dplyr)
library(tidyr)

# Set the working directory to the location of your files
setwd("~/Desktop/University/CSN-MIRI Complex and Social Networks/Lab/Lab4/data")

# List all the files in the directory
files <- list.files(pattern = "*.txt")

# Initialize an empty data frame to store results
results <- data.frame()

# Loop through each file and process the data
for(file in files) {
  
  # Read the data from the file
  data <- read.table(file, header = FALSE)
  colnames(data) <- c("n", "k2")
  
  # Calculate sample size, mean, and standard deviation for n, and k2
  N <- length(data$n)#correct
  mu_n <- mean(data$n)#correct
  sigma_n <- sum((data$n - mu_n) ** 2)/ N #correct
  mu_k2 <- mean(data$k2)#correct
  sigma_k2 <- sum((data$k2-mu_n)**2)/N#correct
  
  # Check validity of k2 based on provided criteria
  valid_k2 <- all(data$k2 >= 4 - 6/data$n & data$k2 <= data$n - 1)
  
  # Add results to the results data frame
  results <- rbind(results, cbind(Language = gsub(".txt", "", file), N, mu_n, sigma_n, mu_k2, sigma_k2, valid_k2))
  
}

# Convert results to a data frame
results <- as.data.frame(results)

# Print the results
print(results)

```

3.Data Analysis
We load the information about a collection of dependency trees from sentence in 
Catalan and sort the matrix rows by the number of vertices increasingly.
```{r}
setwd("~/Desktop/University/CSN-MIRI Complex and Social Networks/Lab/Lab4")
Catalan = read.table("./data/Catalan_dependency_tree_metrics.txt", header = FALSE)
colnames(Catalan) = c("vertices","degree_2nd_moment", "mean_length")
Catalan = Catalan[order(Catalan$vertices), ]
```

3.1 Preliminary visualization
We consider the mean dependency length and create a preliminary plot once and
again on both axes. We see that a power-law dependency between mean length
and number of vertices is suggested in spite of the high dispersion.
When then obtain a clearer intuition about the underlying trend that is obtained
by averaging the mean lengths for a given number of vertices. We compare some 
plots and gain an intuition about how far the real scaling of <d> is from a
random linear arrangement by adding the expected mean length in that case to the
plots. For this we consider a plot in double log scale with the averaged curve 
and the random linear arrangement expectation. 
For the scaling of <k^2> a suitable null model are uniformly distributed random undirected trees. f(n) can be estimated numerically by producing many of those 
trees for a given n. These uniformly randomly labelled spanning trees are 
generatedwith the the Aldous-Brother algorithm. An initial exploration of the 
scaling of <k^2> is performed with its expected value in uniformly random trees 
and the theoretical lower and upper bounds. These plots then suggest that bot 
<d> and <k^2> grow sublinearly with n. 
```{r}
#1
png("31plots/vertices_meandependencylengths.png")

plot(Catalan$vertices, Catalan$mean_length,
       xlab = "vertices", ylab = "mean dependency length")

dev.off()

#2
png("31plots/logvertices_logmeandependencylengths.png")

plot(log(Catalan$vertices), log(Catalan$mean_length),
       xlab = "log(vertices)", ylab = "log(mean dependency length)")

dev.off()

#3
mean_Catalan = aggregate(Catalan, list(Catalan$vertices), mean)

#4
png("31plots/vertices_meanmeandependencylength.png")

plot(mean_Catalan$vertices, mean_Catalan$mean_length,
       xlab = "vertices", ylab = "mean mean dependency length")

dev.off()

#5
png("31plots/logvertices_logmeanmeandependencylength.png")

plot(log(mean_Catalan$vertices), log(mean_Catalan$mean_length),
       xlab = "log(vertices)", ylab = "log(mean mean dependency length)")

dev.off()

#6
png("31plots/vertices_meanmeandependencylength_greenred.png")

plot(log(Catalan$vertices), log(Catalan$mean_length),
       xlab = "vertices", ylab = "mean dependency length")
lines(log(mean_Catalan$vertices),log(mean_Catalan$mean_length), col = "green")
lines(log(mean_Catalan$vertices),log((mean_Catalan$vertices+1)/3), col = "red")

dev.off()

#7

png("31plots/vertices_degree2ndmoment_greenredblue.png")

plot(Catalan$vertices, Catalan$degree_2nd_moment,
       xlab = "vertices", ylab = "degree 2nd moment")
lines(mean_Catalan$vertices,mean_Catalan$degree_2nd_moment, col = "green")
lines(Catalan$vertices,
       (1 - 1/Catalan$vertices)*(5 - 6/Catalan$vertices), col = "red")
lines(Catalan$vertices,4-6/Catalan$vertices, col = "blue")
lines(Catalan$vertices,Catalan$vertices-1, col = "blue")

dev.off()
```

3.2 Then ensemble of models
An ensemble of four models is considered. Furthermore a null model is
considered as model 0. The four models are then generalized with a d term.

4.Non-linear regression with R
Here we fit the models to <k^2> as a function of n.The non-linear regression is
invoked by nls(). Good initial values for a and b can be obtained with a double
logarithmic transformation:log ⟨d⟩ = b log n + a′. Thereafter we perform a non
linear regression to retrief the parameters of the linear model. There after 
we receive the RSS and the AIC. Lastyly the parameters giving the best fit are 
obtained.

Doing this for all models with parameters
```{r}
library(lmtest)

#Find initial values
linear_model = lm(log(degree_2nd_moment)~log(vertices), Catalan)
a_initial = exp(coef(linear_model)[1])
b_initial = coef(linear_model)[2]

#Run the nls() for the first time
nonlinear_model = nls(degree_2nd_moment~a*vertices^b,data=Catalan,
  start = list(a = a_initial, b = b_initial), trace = FALSE)
nonlinear_model#debug
class(nonlinear_model)#debug
#terms(nonlinear_model)#debug

residuals_nonlinear_model <- residuals(nonlinear_model)
fitted_nonlinear_model <- fitted(nonlinear_model)
# Create a linear model for the squared residuals
lm_resid <- lm(residuals_nonlinear_model^2 ~ fitted_nonlinear_model)
# Perform the Breusch-Pagan test
bp_test_result <- bptest(lm_resid)
# Print the test result
print(bp_test_result)#debug
if(bp_test_result < 0.05){#if lower than 0.05 indicating the presence of heteroscedasticity,  take aggregated values
  aggregation <- aggregate(. ~ vertices, data = Catalan, mean)
  nonlinear_model <- nls(mean_length ~ a * vertices^b, data = aggregation, start = list(a = a_initial, b = b_initial), trace = TRUE)
}

#calc the values
deviance(nonlinear_model)#rss
AIC(nonlinear_model)#AIC
sqrt(deviance(nonlinear_model)/df.residual(nonlinear_model))#s
coef(nonlinear_model)["a"]#a
coef(nonlinear_model)["b"]#b
```


Trial below
```{r}
#1 Replacing mean_length by degree_2nd_moment
# nls invokes the non-linear regression
# degree_2nd_moment~a*vertices^b is the mathematical definition of the function
# to fit
#data = Catalan is the data that is being fitted
#start = list(a = a_initial, b = b_initial) this is the initial value of the
#parameters
# trace = TRUE indicated that the progress of the optiimization alogrithm shall
#be shown

a_initial = 4
b_initial = 4
nonlinear_model = nls(degree_2nd_moment~a*vertices^b,data=Catalan,
  start = list(a = a_initial, b = b_initial), trace = FALSE)

#2 This finds good initial values for the non-linear regression
#lm() is used to perform a non linear regression

linear_model = lm(log(degree_2nd_moment)~log(vertices), Catalan)
a_initial = exp(coef(linear_model)[1])
b_initial = coef(linear_model)[2]

#3
nonlinear_model = nls(degree_2nd_moment~a*vertices^b,data=Catalan,
  start = list(a = a_initial, b = b_initial), trace = FALSE)

#Make sure that homocesdasticity holds, if not use aggregate() and not the 
#original data

#4 Obtain RSS, AIC, S, the best fit, the best fit for a and the best fit for b
deviance(nonlinear_model)
AIC(nonlinear_model)
sqrt(deviance(nonlinear_model)/df.residual(nonlinear_model))
coef(nonlinear_model)
coef(nonlinear_model)["a"]
coef(nonlinear_model)["b"]

#5 Calculating relevant results for models with no parameters

RSS <- sum((Catalan$degree_2nd_moment-(Catalan$vertices+1)/3)^2)
RSS
n <- length(Catalan$vertices)
n
p <- 0
s <- sqrt(RSS/(n - p))
s

#6 Calculating the corresponding AIC

AIC <- n*log(2*pi) + n*log(RSS/n) + n + 2*(p + 1)
AIC
```


ChatGPT Implementation below
```{r}
# Load necessary libraries
if (!require("stats")) install.packages("stats", dependencies=TRUE)
library(stats)

# Assume 'Catalan' is your data frame and it has been loaded correctly
#Catalan <- read.csv("your_data_file.csv")

# Initial values for parameters a and b
a_initial = 4
b_initial = 4

# Non-linear regression model
nonlinear_model = nls(degree_2nd_moment ~ a * vertices^b, data = Catalan,
                      start = list(a = a_initial, b = b_initial), trace = TRUE)

# Perform a linear regression on the logarithmic transformation
linear_model = lm(log(degree_2nd_moment) ~ log(vertices), data = Catalan)

# Obtain initial values from the linear model
a_initial = exp(coef(linear_model)[1])
b_initial = coef(linear_model)[2]

# Run the non-linear regression again with new initial values
nonlinear_model = nls(degree_2nd_moment ~ a * vertices^b, data = Catalan,
                      start = list(a = a_initial, b = b_initial), trace = TRUE)

# Calculate RSS, AIC, and standard error of residuals
RSS <- deviance(nonlinear_model)
AIC_value <- AIC(nonlinear_model)
s <- sqrt(RSS / df.residual(nonlinear_model))

# Print the results
cat("RSS:", RSS, "\n")
cat("AIC:", AIC_value, "\n")
cat("Standard Error of Residuals:", s, "\n")

# Extracting best fit parameters
best_fit_params <- coef(nonlinear_model)
cat("Best fit for a:", best_fit_params["a"], "\n")
cat("Best fit for b:", best_fit_params["b"], "\n")

# Calculate RSS, s2 for the model ⟨d⟩ = (n + 1)/3
RSS_0 <- sum((Catalan$degree_2nd_moment - (Catalan$vertices + 1) / 3)^2)
n <- length(Catalan$vertices)
p <- 0
s2 <- RSS_0 / (n - p)

# Calculate AIC for the model ⟨d⟩ = (n + 1)/3
AIC_0 <- n * log(2 * pi) + n * log(RSS_0 / n) + n + 2 * (p + 1)

# Print the results for the model ⟨d⟩ = (n + 1)/3
cat("\nModel ⟨d⟩ = (n + 1)/3:\n")
cat("RSS:", RSS_0, "\n")
cat("s^2:", s2, "\n")
cat("AIC:", AIC_0, "\n")

```

4.1 Calculating values for models without parameters
```{r}

```


4.2 Calculating the corresponding AIC
```{r}

```


5 Results

5.1 Model Selection
For each metric, prepare:
Seperate tables for s, AIC, AIC Delta
Tables with the values of the parameters ogiving the best fir for each model
```{r}

```


5.2 Final visualization
For each language plot the empirical data and the curve for the best fit. 
```{r}
#Example from assignment sheet
plot(log(Catalan$vertices), log(Catalan$mean_length),
       xlab = "log(vertices)", ylab = "log(mean dependency length)")
lines(log(Catalan$vertices), log(fitted(nonlinear_model)), col = "green")
```

